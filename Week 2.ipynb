{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRADIENT DESCENT ALGORITHM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Gradient Descent Algorithm on a toy 2-D dataset which consists of 40 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w,b,x):\n",
    "    return 1.0/(1.0+np.exp(-(w*x+b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the squared error loss function to compute error \n",
    "\n",
    "> loss= 0.5 * (y' - y)^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(w,b):# calculate loss/error\n",
    "    err=0.0\n",
    "    for x,y in zip(X,Y):\n",
    "        fx = f(w,b,x)\n",
    "        err += 0.5*(fx-y)**2\n",
    "    return err\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_b(w,b,x,y):\n",
    "    fx=f(w,b,x)\n",
    "    return (fx-y)*fx*(1-fx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_w(w,b,x,y):\n",
    "    fx=f(w,b,x)\n",
    "    return (fx-y)*fx*(1-fx)*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_gradient_descent(X,Y,w,b,eta,max_epochs):# gradient descent update\n",
    "    dw=0\n",
    "    db=0\n",
    "    for i in range(max_epochs):\n",
    "        for x,y in zip(X,Y):\n",
    "            dw+=grad_w(w,b,x,y)\n",
    "            db+=grad_b(w,b,x,y)\n",
    "        w=w-eta*dw\n",
    "        b=b-eta*db\n",
    "        print(\"Epoch{}: Loss={}\".format(i,error(w,b)))\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating loss at the end of 100 iterations.\n",
    "\n",
    "Hyperparameters :\n",
    ">learning rate = 0.01\n",
    "\n",
    ">initial weight = 1\n",
    "\n",
    ">initial bias = 1\n",
    "\n",
    ">number of iterations = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch0: Loss=0.06187936025238457\n",
      "Epoch1: Loss=0.06084980120291369\n",
      "Epoch2: Loss=0.05934117042922423\n",
      "Epoch3: Loss=0.0573940883480627\n",
      "Epoch4: Loss=0.05505923548740178\n",
      "Epoch5: Loss=0.052394801388978\n",
      "Epoch6: Loss=0.04946375874914613\n",
      "Epoch7: Loss=0.04633117077772086\n",
      "Epoch8: Loss=0.043061708283217946\n",
      "Epoch9: Loss=0.039717507350380724\n",
      "Epoch10: Loss=0.036356447465335506\n",
      "Epoch11: Loss=0.033030881232381334\n",
      "Epoch12: Loss=0.029786805843368104\n",
      "Epoch13: Loss=0.026663436072317203\n",
      "Epoch14: Loss=0.02369311936328153\n",
      "Epoch15: Loss=0.020901524495248746\n",
      "Epoch16: Loss=0.01830803433852916\n",
      "Epoch17: Loss=0.015926278030415665\n",
      "Epoch18: Loss=0.013764746272989396\n",
      "Epoch19: Loss=0.011827443532833975\n",
      "Epoch20: Loss=0.01011454129869244\n",
      "Epoch21: Loss=0.008623006277138887\n",
      "Epoch22: Loss=0.007347185914102041\n",
      "Epoch23: Loss=0.00627934066021197\n",
      "Epoch24: Loss=0.005410117906624068\n",
      "Epoch25: Loss=0.004728966604388037\n",
      "Epoch26: Loss=0.004224494426704987\n",
      "Epoch27: Loss=0.0038847711591874037\n",
      "Epoch28: Loss=0.0036975830332984373\n",
      "Epoch29: Loss=0.0036506431611468804\n",
      "Epoch30: Loss=0.0037317632658986776\n",
      "Epoch31: Loss=0.003928991677426178\n",
      "Epoch32: Loss=0.004230722189287912\n",
      "Epoch33: Loss=0.004625777931133417\n",
      "Epoch34: Loss=0.005103473954002944\n",
      "Epoch35: Loss=0.005653661787604944\n",
      "Epoch36: Loss=0.006266758825886948\n",
      "Epoch37: Loss=0.006933765036844108\n",
      "Epoch38: Loss=0.007646269174680959\n",
      "Epoch39: Loss=0.008396446393671262\n",
      "Epoch40: Loss=0.009177048918334967\n",
      "Epoch41: Loss=0.00998139120865048\n",
      "Epoch42: Loss=0.010803330867265454\n",
      "Epoch43: Loss=0.011637246364237902\n",
      "Epoch44: Loss=0.01247801250081414\n",
      "Epoch45: Loss=0.013320974395016462\n",
      "Epoch46: Loss=0.014161920646879846\n",
      "Epoch47: Loss=0.014997056228995057\n",
      "Epoch48: Loss=0.01582297554780473\n",
      "Epoch49: Loss=0.016636636032218183\n",
      "Epoch50: Loss=0.017435332527966776\n",
      "Epoch51: Loss=0.018216672708101737\n",
      "Epoch52: Loss=0.018978553651492307\n",
      "Epoch53: Loss=0.019719139691411296\n",
      "Epoch54: Loss=0.020436841594553275\n",
      "Epoch55: Loss=0.02113029709634945\n",
      "Epoch56: Loss=0.021798352790439447\n",
      "Epoch57: Loss=0.022440047347869575\n",
      "Epoch58: Loss=0.02305459602426412\n",
      "Epoch59: Loss=0.023641376400164393\n",
      "Epoch60: Loss=0.024199915290298732\n",
      "Epoch61: Loss=0.02472987675114712\n",
      "Epoch62: Loss=0.025231051112267384\n",
      "Epoch63: Loss=0.025703344954988112\n",
      "Epoch64: Loss=0.026146771961845557\n",
      "Epoch65: Loss=0.026561444561192873\n",
      "Epoch66: Loss=0.02694756629345299\n",
      "Epoch67: Loss=0.027305424828269562\n",
      "Epoch68: Loss=0.027635385565133837\n",
      "Epoch69: Loss=0.027937885753763377\n",
      "Epoch70: Loss=0.028213429074449195\n",
      "Epoch71: Loss=0.02846258062266759\n",
      "Epoch72: Loss=0.02868596224639048\n",
      "Epoch73: Loss=0.02888424818866265\n",
      "Epoch74: Loss=0.029058160992101633\n",
      "Epoch75: Loss=0.029208467625979846\n",
      "Epoch76: Loss=0.02933597580045206\n",
      "Epoch77: Loss=0.029441530436271592\n",
      "Epoch78: Loss=0.02952601026199295\n",
      "Epoch79: Loss=0.029590324514174857\n",
      "Epoch80: Loss=0.029635409719477222\n",
      "Epoch81: Loss=0.029662226540781576\n",
      "Epoch82: Loss=0.029671756672560556\n",
      "Epoch83: Loss=0.0296649997736699\n",
      "Epoch84: Loss=0.029642970428541963\n",
      "Epoch85: Loss=0.02960669513041111\n",
      "Epoch86: Loss=0.02955720928270284\n",
      "Epoch87: Loss=0.029495554217058115\n",
      "Epoch88: Loss=0.029422774228642225\n",
      "Epoch89: Loss=0.029339913631391844\n",
      "Epoch90: Loss=0.02924801383768159\n",
      "Epoch91: Loss=0.0291481104685324\n",
      "Epoch92: Loss=0.0290412305019322\n",
      "Epoch93: Loss=0.028928389468087887\n",
      "Epoch94: Loss=0.028810588701469717\n",
      "Epoch95: Loss=0.028688812660340363\n",
      "Epoch96: Loss=0.02856402632507788\n",
      "Epoch97: Loss=0.02843717268700243\n",
      "Epoch98: Loss=0.028309170339601564\n",
      "Epoch99: Loss=0.02818091118402103\n",
      "Error=0.02818091118402103\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    filename='A2_Q4_data.csv'\n",
    "    df=pd.read_csv(filename)\n",
    "    X=df['X']\n",
    "    Y=df['Y']\n",
    "    initial_w=1\n",
    "    initial_b=1\n",
    "    eta=0.01\n",
    "    max_epochs=100\n",
    "    w,b=do_gradient_descent(X,Y,initial_w,initial_b,eta,max_epochs)\n",
    "    error=error(w,b)\n",
    "    print(\"Error={}\".format(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
